{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Tharkana Vishmika Indrahenaka\n",
    "#### Student ID: s4065784\n",
    "\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "\n",
    "## Introduction\n",
    "In this assignment, the review text was preprocessed by tokenizing, converting to lowercase, and removing short words, stopwords, rare words, and the top 20 most frequent words, resulting in cleaned data saved in processed.csv and a vocabulary stored in vocab.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment employed the \"Pandas\" library to load and manipulate the dataset (assignment3.csv). The \"re\" module was utilized for tokenizing the review text through regular expressions, ensuring an accurate breakdown of textual data into tokens. Furthermore, \"collections.Counter\" was leveraged to compute word frequencies across the dataset, enabling the identification and subsequent removal of infrequent words and the top 20 most frequently occurring terms. The combined use of these libraries provided an efficient and systematic approach to the text preprocessing workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- Examine the data and explain your findings\n",
    "- Load the data into proper data structures and get it ready for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, This would involve loading the data into a DataFrame, studying the first few rows to get an initial feel for the structure of the data, and being especially attentive to the 'Review Text' column since that is what this whole cleaning and preprocessing task has been for. This will also be useful to take note of what cleaning and transformation may be required on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0         1077   60  Some major design flaws   \n",
       "1         1049   50         My favorite buy!   \n",
       "2          847   47         Flattering shirt   \n",
       "3         1080   49  Not for the very petite   \n",
       "4          858   39     Cagrcoal shimmer fun   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  I had such high hopes for this dress and reall...       3                0   \n",
       "1  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "2  This shirt is very flattering to all due to th...       5                1   \n",
       "3  I love tracy reese dresses, but this one is no...       2                0   \n",
       "4  I aded this in my basket at hte last mintue to...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0         General         Dresses    Dresses  \n",
       "1                        0  General Petite         Bottoms      Pants  \n",
       "2                        6         General            Tops    Blouses  \n",
       "3                        4         General         Dresses    Dresses  \n",
       "4                        1  General Petite            Tops      Knits  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load stopwords\n",
    "with open('stopwords_en.txt', 'r') as file:\n",
    "    stopwords = set(file.read().splitlines())\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('assignment3.csv')\n",
    "\n",
    "# Display data for inspection\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Set of Reviews :  ['I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c', \"I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\", 'This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!']\n"
     ]
    }
   ],
   "source": [
    "reviews = df['Review Text'].tolist()\n",
    "print(\"Sample Set of Reviews : \", reviews[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 19662\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of reviews: {len(reviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on basic text pre-processing\n",
    "\n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code checks for **missing** or **null value**s in each column of the DataFrame, helping to identify any potential issues in the data that need to be addressed before further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing ID                0\n",
      "Age                        0\n",
      "Title                      0\n",
      "Review Text                0\n",
      "Rating                     0\n",
      "Recommended IND            0\n",
      "Positive Feedback Count    0\n",
      "Division Name              0\n",
      "Department Name            0\n",
      "Class Name                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in the DataFrame\n",
    "null_values = df.isnull().sum()\n",
    "\n",
    "# Display the columns with their corresponding count of null values\n",
    "print(null_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess_text function cleans and tokenizes the review text by splitting it into words based on a **regex pattern**, **converting tokens to lowercase**, **removing words shorter than two character**s, and **filtering out stopwords**. This prepares the text for meaningful analysis by ensuring it is standardized and free from irrelevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing the review text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize using regex\n",
    "    tokens = re.findall(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\", text)\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Filter out short words\n",
    "    tokens = [token for token in tokens if len(token) >= 2]\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing function is applied to each review, creating a new column in the DataFrame with cleaned tokens, making the data ready for further analysis such as word frequency counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "df['Processed_Review'] = df['Review Text'].apply(preprocess_text)\n",
    "print(\"Preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rare and Frequent Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tokens from the Processed_Review column are combined into a single list, and their frequencies are counted using collections.Counter. This step helps in determining which words are rare and which are frequently occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count term frequencies across all documents\n",
    "all_tokens = [token for tokens in df['Processed_Review'] for token in tokens]\n",
    "word_counts = Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Processed_Review column is updated to exclude tokens that occur only once across the entire dataset. **Removing such rare words** reduces noise and helps in focusing on more meaningful tokens that appear frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words that appear only once\n",
    "df['Processed_Review'] = df['Processed_Review'].apply(\n",
    "    lambda tokens: [token for token in tokens if word_counts[token] > 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering out rare words, **word frequencie**s are recomputed to find the top 20 most frequent tokens. This helps in identifying commonly occurring words that may not contribute significantly to the context and should be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute word frequencies to find the top 20 most frequent words\n",
    "all_tokens = [token for tokens in df['Processed_Review'] for token in tokens]\n",
    "word_counts = Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 20 most frequent words are picked based on document frequency and then removed from each review in the Processed_Review column. This step strengthens the focus towards more contextually relevant words by removing the very common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and remove the top 20 most frequent words\n",
    "top_20_words = [word for word, _ in word_counts.most_common(20)]\n",
    "df['Processed_Review'] = df['Processed_Review'].apply(\n",
    "    lambda tokens: [token for token in tokens if token not in top_20_words]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned tokens are converted back to space-separated strings within the Processed_Review column. This transformation prepares the processed text for output and further analysis in a readable and usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted processed reviews to strings.\n"
     ]
    }
   ],
   "source": [
    "# Convert lists of tokens to space-separated strings\n",
    "df['Processed_Review'] = df['Processed_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "print(\"Converted processed reviews to strings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the requested information as per specification.\n",
    "- vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves the preprocessed reviews to a new CSV file, entitled processed.csv, containing only the Processed_Review column. The index = False ensures that the saved file doesn't save the DataFrame index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to 'processed.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the processed data to 'processed.csv'\n",
    "df[['Processed_Review']].to_csv('processed.csv', index=False)\n",
    "print(\"Processed data saved to 'processed.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it picks up all the unique tokens from the Processed_Review column and creates a sorted vocabulary. The sorted vocabulary will save as a dictionary in which each word will be mapped to a unique index starting from 0. The vocabulary is written to a text file called vocab.txt, where each line consists of a word in the vocabulary, along with its index in the format word:index. It would serve as a key to understand what the token means in the reviews that have gone through preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary of the cleaned reviews\n",
    "vocab = sorted(set(all_tokens))  # Sort vocabulary alphabetically\n",
    "vocab_dict = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "# Save to 'vocab.txt' in the correct format\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for word, index in vocab_dict.items():\n",
    "        f.write(f\"{word}:{index}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The preprocessed dataset cleaned and transformed the review text in this task. First, it tokenized the text by using a regular expression and changed all words to lowercase. Then, it removed words that were less than two characters and stopwords to keep only meaningful contents. Besides, rare words-words that appeared just once across the reviews-and the top 20 most frequent words have been filtered out in order to reduce the noise and increase the quality of the data. Save cleaned reviews into a new file called processed.csv which will be further used in feature generation and modeling tasks. A sorted vocabulary of the rest of the tokens was further created and saved to vocab.txt, providing a structured way of referring to the conversion of textual data into vectorized formats for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
